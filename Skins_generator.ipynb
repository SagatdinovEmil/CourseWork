{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe677ng2Q28a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eade3a4-76fa-430f-a920-e3785c7496d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "# !pip install opencv-python-headless==4.5.2.52"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdu3-_USONbe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Импортируем необходимые библиотеки для создания генеративно-состязательной сети\n",
        "Код разработан в основном с использованием библиотеки PyTorch\n",
        "\"\"\"\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.io import read_image, write_png\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "from torchvision.utils import save_image\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from random import triangular\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ic5gIrzPTGE"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.random import Tensor\n",
        "batch_size = 24\n",
        "\n",
        "def rgba_loader(path) -> Tensor:\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        \n",
        "        return (pil_to_tensor(img.convert('RGBA')).float() / 255 * 2) - 1\n",
        "\n",
        "dataset = ImageFolder(f\"/content/drive/MyDrive/SkinsTrimmed\", loader = rgba_loader)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFfMAVSOQhR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Определяем, доступны ли какие-либо графические процессоры\n",
        "\"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgicVmPVOSuv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Сетевые архитектуры\n",
        "Ниже приведены архитектуры дискриминатора и генератора\n",
        "\"\"\"\n",
        "class DisConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "      super(DisConvBlock, self).__init__()\n",
        "      self.conv = spectral_norm(torch.nn.Conv2d(in_channels=in_channels,\n",
        "                                  out_channels=out_channels,\n",
        "                                  kernel_size=kernel_size, padding=padding,\n",
        "                                  bias=False))\n",
        "      self.norm = nn.BatchNorm2d(out_channels)\n",
        "      self.act = nn.LeakyReLU(0.1)\n",
        "      self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = self.act(x)\n",
        "      x = self.drop(x)\n",
        "      return x\n",
        "\n",
        "class GenConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "      super(GenConvBlock, self).__init__()\n",
        "      self.conv = torch.nn.ConvTranspose2d(in_channels=in_channels,\n",
        "                                  out_channels=out_channels,\n",
        "                                  kernel_size=kernel_size, padding = padding,\n",
        "                                  bias=True)\n",
        "      self.norm = nn.BatchNorm2d(out_channels)\n",
        "      self.act = nn.ReLU(0.1)\n",
        "      self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = self.norm(x)\n",
        "      x = self.act(x)\n",
        "      x = self.drop(x)\n",
        "      return x\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "      super(SelfAttention, self).__init__()\n",
        "      self.f = torch.nn.Conv2d(in_channels, in_channels//8, 1)\n",
        "      self.g = torch.nn.Conv2d(in_channels, in_channels//8, 1)\n",
        "      self.h = torch.nn.Conv2d(in_channels, in_channels, 1)\n",
        "      self.gamma = nn.Parameter(torch.zeros(1))\n",
        "      self.softmax  = nn.Softmax(dim=-1)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    batch_size, C, width, height = x.size()\n",
        "    proj_query  = self.f(x).view(batch_size,-1,width*height).permute(0, 2, 1)\n",
        "    proj_key =  self.g(x).view(batch_size,-1,width*height)\n",
        "    energy =  torch.bmm(proj_query,proj_key)\n",
        "    attention = self.softmax(energy)\n",
        "    proj_value = self.h(x).view(batch_size,-1,width*height)\n",
        "\n",
        "    out = torch.bmm(proj_value, attention)\n",
        "    out = out.view(batch_size,C,width,height)\n",
        "    \n",
        "    out = self.gamma*out + x\n",
        "    return out\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.conv_block1 = DisConvBlock(4, 256, 5, 0)\n",
        "        self.conv_block2 = DisConvBlock(256, 512, 5, 0)\n",
        "        self.conv_block3 = DisConvBlock(512, 1024, 5, 0)\n",
        "        self.attn = SelfAttention(1024)\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channels=64, out_channels=1,\n",
        "                               kernel_size=52))\n",
        "        self.act = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.conv_block4(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(-1, 1)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(generator, self).__init__()\n",
        "        self.deconv_block1 = GenConvBlock(1, 1024, 5, 2)\n",
        "        self.deconv_block2 = GenConvBlock(1024, 512, 5, 2)\n",
        "        self.deconv_block3 = GenConvBlock(512, 256, 5, 2)\n",
        "        self.attn = SelfAttention(256)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 4, 3, padding=1)\n",
        "        self.act4 = nn.Tanh()\n",
        "    def forward(self, x):\n",
        "        x = self.deconv_block1(x)\n",
        "        x = self.deconv_block2(x)\n",
        "        x = self.deconv_block3(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.act4(x)\n",
        "        return (x)\n",
        "    def gen_skin(self, filename):\n",
        "        noise = (torch.rand(1, 1, 64, 64))\n",
        "        noise = noise.to(device)\n",
        "        save_image((self.forward(noise)/2 + 0.5)*255, filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/SkinsGenerator/dis_losses.txt', 'a+') as f:\n",
        "    for number in trainer.dis_losses:\n",
        "        f.write(str(number) + \"\\n\")"
      ],
      "metadata": {
        "id": "Cj9nZuc7Gapm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Bu55fNCXeK"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self):\n",
        "    self.discriminator = discriminator().to(device)\n",
        "    self.generator = generator().to(device)\n",
        "    self.D_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    self.G_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    self.loss = nn.BCELoss()\n",
        "    self.dis_losses = []\n",
        "    self.gen_losses = []\n",
        "    self.current_epoch = 0\n",
        "  def load_model(self):\n",
        "    G_checkpoint = torch.load(\"/content/drive/MyDrive/SkinsGenerator/Generators/Generator.pth\")\n",
        "    D_checkpoint = torch.load(\"/content/drive/MyDrive/SkinsGenerator/Discriminators/Discriminator.pth\")\n",
        "    self.discriminator.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "    self.D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "    self.generator.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "    self.G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "    self.current_epoch = D_checkpoint['epoch']\n",
        "  def save_losses_to_file(self):\n",
        "    with open('/content/drive/MyDrive/SkinsGenerator/dis_losses.txt', 'a+') as f:\n",
        "      for number in self.dis_losses:\n",
        "          f.write(str(number) + \"\\n\")\n",
        "      f.close()\n",
        "    with open('/content/drive/MyDrive/SkinsGenerator/gen_losses.txt', 'a+') as f:\n",
        "      for number in self.gen_losses:\n",
        "          f.write(str(number) + \"\\n\")\n",
        "      f.close()\n",
        "  def save_control_point(self):\n",
        "    torch.save({'epoch': epoch,\n",
        "              'model_state_dict': self.generator.state_dict(),\n",
        "              'optimizer_state_dict': self.G_optimizer.state_dict()},\n",
        "              '/content/drive/MyDrive/SkinsGenerator/Generators/Generator.pth')\n",
        "    torch.save({'epoch': epoch,\n",
        "              'model_state_dict': self.discriminator.state_dict(),\n",
        "              'optimizer_state_dict': self.D_optimizer.state_dict()},\n",
        "              '/content/drive/MyDrive/SkinsGenerator/Discriminators/Discriminator.pth')\n",
        "    print('Model saved.')\n",
        "  def train(self, data_loader, crit, numbers_of_epoch, save_frequency):\n",
        "    torch.set_grad_enabled(True) \n",
        "    start_epoch = self.current_epoch\n",
        "    for p in self.discriminator.parameters():  # reset requires_grad\n",
        "        p.requires_grad = True  # they are set to False below in netG update\n",
        "    for epoch in range(start_epoch, start_epoch+numbers_of_epoch):\n",
        "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "        for i, data in progress_bar:\n",
        "            imgs = data[0]\n",
        "            # Adversarial ground truths\n",
        "            valid = Variable((torch.rand((imgs.shape[0], 1))+4)/5, requires_grad=False).to(device)\n",
        "            fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False).to(device)\n",
        "\n",
        "            # Configure input\n",
        "            real_imgs = imgs.to(device)\n",
        "\n",
        "            z = Variable(Tensor((torch.rand(imgs.shape[0], 1, 64, 64)))).to(device)\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = self.generator(z)\n",
        "\n",
        "\n",
        "            self.D_optimizer.zero_grad()\n",
        "            real_loss = self.loss(self.discriminator(real_imgs), valid)\n",
        "            fake_loss = self.loss(self.discriminator(gen_imgs.detach()), fake)\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            if (i + 1) % crit == 0:\n",
        "              for p in self.discriminator.parameters():  # reset requires_grad\n",
        "                p.requires_grad = True  # they are set to False below in netG update\n",
        "\n",
        "              # Measure discriminator's ability to classify real from generated samples\n",
        "              \n",
        "              real_loss = self.loss(self.discriminator(real_imgs), valid)\n",
        "              real_loss.backward()\n",
        "              fake_loss = self.loss(self.discriminator(gen_imgs.detach()), fake)\n",
        "              fake_loss.backward()\n",
        "              self.D_optimizer.step()\n",
        "              #d_loss = (real_loss + fake_loss) / 2\n",
        "          \n",
        "            \n",
        "            for p in self.discriminator.parameters():  # reset requires_grad\n",
        "              p.requires_grad = False  # they are set to False below in netG update\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            self.G_optimizer.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = Variable(Tensor((torch.rand(imgs.shape[0], 1, 64, 64)))).to(device)\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = self.generator(z)\n",
        "\n",
        "            # Loss measures generator's ability to fool the discriminator\n",
        "            g_loss = self.loss(self.discriminator(gen_imgs), valid)\n",
        "            g_loss.backward()\n",
        "            self.G_optimizer.step()\n",
        "\n",
        "            progress_bar.set_description(f\"[{epoch + 1}/{start_epoch+numbers_of_epoch}][{i + 1}/{len(dataloader)}] \"\n",
        "                                          f\"Loss_D: {(real_loss + fake_loss) / 2:.6f} Loss_G: {g_loss:.6f} \")\n",
        "            self.dis_losses.append(float(real_loss + fake_loss))\n",
        "            self.gen_losses.append(float(g_loss))\n",
        "            \n",
        "        if (epoch+1) % save_frequency  == 0:\n",
        "            self.save_control_point()\n",
        "            for i in range(9):\n",
        "              self.generator.gen_skin(f\"/content/drive/MyDrive/SkinsGenerator/Generated_imgs/epoch_{epoch+1}_{i}.png\")\n",
        "    self.save_losses_to_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz3W0SsKMpym"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VPrXrrbOyTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "acfe3f1e-a717-4c56-f69c-af7f6836790a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1/200][415/415] Loss_D: 0.216459 Loss_G: 4.441345 : 100%|██████████| 415/415 [03:14<00:00,  2.13it/s]\n",
            "[2/200][415/415] Loss_D: 0.199475 Loss_G: 6.055211 : 100%|██████████| 415/415 [03:08<00:00,  2.20it/s]\n",
            "[3/200][415/415] Loss_D: 0.179310 Loss_G: 5.092997 : 100%|██████████| 415/415 [03:09<00:00,  2.19it/s]\n",
            "[4/200][415/415] Loss_D: 0.204162 Loss_G: 4.615445 : 100%|██████████| 415/415 [03:09<00:00,  2.19it/s]\n",
            "[5/200][415/415] Loss_D: 0.181685 Loss_G: 5.016153 : 100%|██████████| 415/415 [03:09<00:00,  2.19it/s]\n",
            "[6/200][144/415] Loss_D: 0.194663 Loss_G: 5.091354 :  35%|███▍      | 144/415 [01:06<02:04,  2.17it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c30e10e7dbde>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-fe026fc79c94>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, crit, numbers_of_epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             progress_bar.set_description(f\"[{epoch + 1}/{start_epoch+numbers_of_epoch}][{i + 1}/{len(dataloader)}] \"\n\u001b[0m\u001b[1;32m     81\u001b[0m                                           f\"Loss_D: {(real_loss + fake_loss) / 2:.6f} Loss_G: {g_loss:.6f} \")\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdis_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train(dataloader, 5, 200, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJ6-uE4Cfp7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}